%% This file is modified by Veli Mäkinen from HY_fysiikka_LuKtemplate.tex authored by Roope Halonen ja Tomi Vainio.
%% Some text is also inherited from engl_malli.tex by Kutvonen, Erkiö, Mäkelä, Verkamo, Kurhila, and Nykänen.


% STEP 1: Choose oneside or twoside
\documentclass[finnish,twoside,openright]{HYgraduMLDS}
%finnish,swedish

%\usepackage[utf8]{inputenc} % For UTF8 support. Use UTF8 when saving your file.
\usepackage{lmodern} % Font package
\usepackage{textcomp} % Package for special symbols
\usepackage[pdftex]{color, graphicx} % For pdf output and jpg/png graphics
\usepackage[pdftex, plainpages=false]{hyperref} % For hyperlinks and pdf metadata
\usepackage{fancyhdr} % For nicer page headers
\usepackage{tikz} % For making vector graphics (hard to learn but powerful)
%\usepackage{wrapfig} % For nice text-wrapping figures (use at own discretion)
\usepackage{amsmath, amssymb} % For better math
%\usepackage[square]{natbib} % For bibliography
\usepackage[footnotesize,bf]{caption} % For more control over figure captions
\usepackage{blindtext}
\usepackage{titlesec}
\usepackage[titletoc]{appendix}
\usepackage{hyperref}

\onehalfspacing %line spacing
%\singlespacing
%\doublespacing

%\fussy 
\sloppy % sloppy and fussy commands can be used to avoid overlong text lines

% STEP 2:
% Set up all the information for the title page and the abstract form.
% Replace parameters with your information.
\title{SQL:n Oppiminen Datatieteen Menetelmin}
\author{Matti Räty}
\date{\today}
\prof{Professor X or Dr. Y}
\censors{Professor A}{Dr. B}{}
\keywords{layout, summary, list of references}
\depositeplace{}
\additionalinformation{}


\classification{\protect{\ \\
\  General and reference $\rightarrow$ Document types  $\rightarrow$ Surveys and overviews\  \\
\  Applied computing  $\rightarrow$ Document management and text processing  $\rightarrow$ Document management $\rightarrow$ Text editing\\
}}

% if you want to quote someone special. You can comment this line and there will be nothing on the document.
%\quoting{Bachelor's degrees make pretty good placemats if you get them laminated.}{Jeph Jacques} 


% OPTIONAL STEP: Set up properties and metadata for the pdf file that pdfLaTeX makes.
% But you don't really need to do this unless you want to.
\hypersetup{
    bookmarks=true,         % show bookmarks bar first?
    unicode=true,           % to show non-Latin characters in Acrobatâs bookmarks
    pdftoolbar=true,        % show Acrobatâs toolbar?
    pdfmenubar=true,        % show Acrobatâs menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={},            % title
    pdfauthor={},           % author
    pdfsubject={},          % subject of the document
    pdfcreator={},          % creator of the document
    pdfproducer={pdfLaTeX}, % producer of the document
    pdfkeywords={something} {something else}, % list of keywords for
    pdfnewwindow=true,      % links in new window
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=black,        % color of internal links
    citecolor=black,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

\begin{document}

% Generate title page.
\maketitle

% STEP 3:
% Write your abstract (of course you really do this last).
% You can make several abstract pages (if you want it in different languages),
% but you should also then redefine some of the above parameters in the proper
% language as well, in between the abstract definitions.

\begin{abstract}
Summary of the main contents of the work: topic, methodology and results.

Topics are classified according to the ACM Computing Classification System
(CCS): check command \verb+\classification{}+. A small set of paths (1-3) should be used, starting from any top nodes
referred to bu the root term CCS leading to the leaf nodes. The elements
in the path are separated by right arrow, and emphasis of each element individually can be indicated
by the use of bold face for high importance or italics for intermediate
level. The combination of individual boldface terms may give the reader
additional insight. 
\end{abstract}

% Place ToC
\mytableofcontents

\mynomenclature

% -----------------------------------------------------------------------------------
% STEP 4: Write the thesis.
% Your actual text starts here. You shouldn't mess with the code above the line except
% to change the parameters. Removing the abstract and ToC commands will mess up stuff.
\chapter{Johdanto}

The thesis should have an introduction chapter. Other chapters can be named according to the topic. In the end, some summary chapter is needed; see Chapter~\ref{chapter:Loppusanat} for an example.

\section{Tutkimuskysymykset}

\begin{itemize}
    \item Miten SQL:n oppimista on tutkittu?
    \begin{itemize}
        \item kirjallisuus
    \end{itemize}
    \item Millä tavoin kurssin tehtävien perusteella ennustaa oppimenestystä?
    \begin{itemize}
        \item Kirjallisuus
    \end{itemize}
    \item Millä tavoin SQL-kurssin oppimenestystä ennustaa kurssin tehtävien perusteella?
    \begin{itemize}
        \item kirjallisuus
        \item tämä data
    \end{itemize}
\end{itemize}


\chapter{Taustaa}

\section{SQL:n oppiminen}

SQL on laajasti hyväksytty kieli tietokantojen hallinnoimiseen niin akateemisissa kuin it-teollisuuden piirissä. Näennäisestä yksinkertaisesta syntaksistaan huolimatta se on voimakas kyselykieli, joka mahdollistaa monimutkaisiakin kyselyjä tietokantaan. SQL komennot sisällyttävät mahdollisuudet määritellä, manipuloida sekä hallinnoida dataa. Luonteeltaan SQL on deklaratiivinen, joka tarkoittaa sitä että kyselyjä kirjoittaessa on ajateltava kokoelmissa. Kyselyt käytännössä kuvailevat haluttua lopputulosta laskematta sitä ennen itse kyselyn suorittamista.

Laajan käytön vuoksi SQL on helposti sovellettavissa opintojen ulkopuolella ja on näin yksi välttämättömämmistä oppiaineista IT-alalla. Laajasta käytöstä huolimatta SQL vaikuttaa olevan vaikea oppia. Vaikeuksien on arvioitu johtuvan SQL:n deklaratiivisesta luonteesta, ja SELECT-kyselyjen näennäisestä yksinkertaisuudesta\cite{sadiq2004sqlator}.

Tietotekniikka-alalla SQL-opinnot ovat kuuluneet IT-alan opintoihin vuosikymmenien ajan. Pitkään historaan nähden, SQL:n opetus on saanut varsin vähäistä huomiota tieteellisissä piireissä\cite{Taipalus:2019:EFS:3287324.3287359}. Esimerkiksi aiheesta että mikä on SQL:n opiskelussa vaikeaa. SQL:n vaikeiden osa-alueiden tutkimuksen tulokset ovat tulkinnanvaraisia\cite{Taipalus:2019:EFS:3287324.3287359}. Perinteisesti SQL:n opetuksen tutkimus on kohdistunut opetustyökaluihin ja opiskelijoiden tekemiin virheisiin\cite{Taipalus:2019:EFS:3287324.3287359}. Opetustyökaluihin lukeutuvat esimerkiksi erilaiset e-oppimisohjelmistot.


\subsection{SQL opetustyökalut}

Parhaiten SQL:n oppimisen on havaittu toteutuvan ohjatuissa opetussessioissa ja siten että opiskelijoilla on mahdollisuus huomata virheitään ja oppia virheistään\cite{sadiq2004sqlator}. Yksi tapa näiden saavuttamiseen ovat tehtäväkokoelmat, jotka tarkistaa SQL:n ammattilainen. Ratkaisuna tämä on työläs muun muassa suuren opiskelijamäärän ja itse tuloksen tarkistamisen vaikeuden vuoksi. Kyselyjen tarkastaminen voi olla hidasta, sillä saman taulun saamiseen voidaan kirjoittaa erilaisia kyselyitä. Ratkaisun ollessa työläs vaarantuu opiskelijoiden saama yksilöllinen palaute. Näin perustellaan SQL e-opetustyökalujen kehittämistä.

E-oppiminen on ollut tutkimusaiheena kasvavassa suosiossa ainakin vuodesta 2000 alkaen. Se on herättänyt kiinnostusta niin työmaailmassa kuin akateemisissa piireissä. E-oppimista perustellaan mahdollisuudella oppia aktiivisesti tekemällä, sen sijaan että opiskellaan asioita passiivisesti lukemalla\cite{Brusilovsky:2010:LSP:1656255.1656257}. E-oppiminen on jaoteltavissa kolmeen luokkaan\cite{sadiq2004sqlator}: hallintajärjestelmiin (management system), yhteisölliseen oppimiseen (collaborative learning) ja kohdennettuihin työkaluihin. Hallintajärjestelmiin lukeutuvat esimerkiksi sovellukset WebCT ja Blackboard. Yhteisölliseen oppimiseen kuuluu erilaiset kommunikointitavat, esimerkiksi foorumit, video-konferenssit tai sähköposti. Kohdennetut työkalut ovat puolestaan esimerkiksi simulaattoreita, interaktiivisia työkaluja sekö monikysymys-tietopankkeja (Multiple Choice Question bank). SQL e-oppimistyökalut, kuten esimerkiksi SQLator tai AsseSQL kuuluvat kolmanteen luokkaan.

Kun harkitaan e-oppimisen käyttöön ottoa on huomioitava muun muassa onko teknologian käyttö pedagogisesti ja opetuksellisesti arvokasta käyttökohteessa\cite{sadiq2004sqlator}. Olennaista itse työkalun kehityksessä on käyttökohteen huomioiminen suunnittelussa siten että se tukee oppimista mahdollisimman hyvin. Esimerkiksi luonteva navigointi opetusympäristössä parantaa merkittävästi opetustyökalun käyttöastetta ja sen tarjoamaa hyötyä\cite{Brusilovsky:2010:LSP:1656255.1656257}. Koska e-oppimistyökalut korostavat itseopiskelua, korostuu opettajan läsnäolon puutteessa tarve ymmärtää opiskelijan käsitystä oppimisesta\cite{sadiq2004sqlator}.

Kehitettyjen e-oppimistyökalujen on todettu parantavan opiskelijoiden kurssiarvosanaa, ja saavat käyttäjiltään hyvää palautetta\cite{Brusilovsky:2010:LSP:1656255.1656257}. Suuntaa antava vertailu SQL:n kontekstissa saatiin vertailemalla kahden perättäisen vuoden tenttiarvosanajakaumaa, jossa jälkimmäisenä vuotena käyttöön otettiin SQLator\cite{sadiq2004sqlator}. Tenttiarvosanajakauma parani vuonna jona SQLator otettiin käyttöön. Onnistuneesti toteutettuna e-oppimistyökalut tukee opiskelijoittensa syväoppimista opettamastaan aiheestaan\cite{sadiq2004sqlator}.

Kohdennetut e-oppimistyökalut jotka on toteutettu SQL:n oppimiseen, vähentävät ihmisen työn tekemää määrää vaarantamatta oppijoiden saamaa palautetta. Koska tarkistuksen tekee oppimistyökalu, tapahtuu kyselyn oikeellisuuden tarkistaminen nopeasti antaen välitöntä palautetta oppijalle. SQLator on esimerkki kohdennetusta e-oppimistyökalusta\cite{sadiq2004sqlator}. Sen kuvaillaan olevan verkko-oppimis työpöytä (online learning workbench). Käytännössä se on interaktiivinen oppiympäristö verkko-pohjaisella käyttöliittymällä, joka mahdollistaa pääsyn oppimisympäristöön kaikkialta internet-yhteyden päästä. SQLatorin kehityksen motivaationa on SQL-kielen laaja käyttö, ja SQL kyselyjen arvioinnin laskennallinen monimutkaisuus. Kyselyjen tarkastamisen ollessa SQLator:n tärkein ominaisuus tarjoaa se lisäksi oppijalle oppimateriaalia, turvallisen ympäristön vapaalle harjoittelulle sekä yhteyden opettajiin. Opettajat saavat mahdollisuuden luoda ja muokata tietokantoja ja tietokantakohtaisia tehtäviä. SQLator kerää opiskelijoilta dataa esimerkiksi lokeja etenemisestä ja tehtävien suoritusyrityksistä, jotka mahdollistavat tilastojen keräämisen ja opiskelijoiden tarkkailun sekä plagiarismin tunnistamisen.

SQLatorin lisäksi SQL-kyselykielen opetteluun on olemassa useita e-oppimistyökaluja. Yksittäinen opetustyökalu ei yksinään välttämättä tarjoa opetuksen vaatimia ominaisuuksia. Yksi mahdollisuus on integroida olemassa olevia työkaluja, joissa tuodaan useampi opetusympäristö yhteen käyttöliittymään. Tutkimus olemassa olevien opetustyökalujen yhdistämiseen on vähäistä, ilmeisesti integroitujen työkalujen teknisestä vaikeudesta johtuen\cite{Brusilovsky:2010:LSP:1656255.1656257}.

Integroitujen työkalujen helppokäyttöisyyden varmistamiseksi on Brusilovskyn ym. \cite{Brusilovsky:2010:LSP:1656255.1656257} mukaan mahdollistettava single sign-on, opiskelijoiden toimintojen seuraaminen ja tallennettujen tietojen saatavuus. Tallennetun tiedon on tärkeää olla muodossa josta opiskelijoiden osaamista on mahdollista päätellä.


\subsection{Tutkimusta SQL:n oppimisesta / Opiskelijoiden virheet SQL:ssä}

Taipalus ym. \cite{Taipalus:2019:EFS:3287324.3287359} tekivät tutkimusta opiskelijoiden tekemistä virheistä SQL-opetuskurssilla. Data kerättiin heidän tekemässä verkkosovelluksessa, jossa tavoite-taulu oli näkyvillä koko tehtävän tekemisen ajan. Kun tehtävän tekijä oli yritysten jälkeen tyytyväinen kyselyynsä, hän palautti vastauksen. Eniten heitä kiinnosti pysyvät virheet (persistent error), eli virheet jotka esiintyivät palautetuissa tehtävissä. 

Taipalus ym. \cite{Taipalus:2019:EFS:3287324.3287359} jakoivat tehdyt virheet neljään yläluokkaan: sekaannuksiin (complication), loogisiin-, semanttisiin- sekä syntaktisiin virheisiin. Sekaannukset eivät vaikuta itse tulostauluun. % tsekkaa niitten toinen artikkeli, jos sais enemmän irti
Loogiset virheet vaikuttavat tulostauluun, mutta tavalla jolla sen tunnistamiseen vaaditaan käsitys ja ymmärrys kyselyn taivoitteesta. Loogisia virheitä ovat esimerkiksi määritelmävirheet (expression error) tai funktiovirheet. Virheet joissa on loogisia virheitä voivat olla hyödyllisiä johonkin toiseen käyttötarkoitukseen. Semanttiset virheet myös vaikuttavat tulostauluun, mutta niiden tunnistaminen on ilmeisempää esimerkiksi toistuvista riveistä. Semanttisesti virheellisillä kyselyillä ei ole käyttökohdetta. Kyselyt joissa on syntaksisia virheitä palauttavat virheilmoituksen ilman vastaustaulua. Syntaktisiin virheisiin lukeutuu esimerkiksi laiton rymittelyfunktio (aggregation-function) tai määrittelemätön tietokantaobjekti.

Yleisin virhe kaikissa SQL kyselyissä ovat syntaktiset virheet\cite{Taipalus:2019:EFS:3287324.3287359, Ahadi:2016:SSM:2839509.2844640}. Luonnollisesti syntaktisten virheiden määrä oli suurempi kurssin alussa, ja vähenivät kurssin edetessä. Ahadi ym. \cite{Ahadi:2016:SSM:2839509.2844640} huomauttivat syntaktisten olevan yleisin virhe, jossa opiskelija päättää luovuttaa jos hän ei saa virhettä ratkaistua. Yleisimmät pysyvien virheiden luokat ovat puolestaan loogiset virheet ja sekaannukset\cite{Taipalus:2019:EFS:3287324.3287359}. Loogiset virheet ovat vaikeita ratkaista, ja tämä oli tapaus myös Taipaluksen ym. \cite{Taipalus:2019:EFS:3287324.3287359} mainitsemalla SQL kurssilla. Loogisten virheiden vaikeuden on ehdotettu juontuvat ihmisen työmuistin luonteeseen, ja tapaan jolla ihminen kääntää ajatuksiaan SQL:ään\cite{SMELCER1995353}. 

Tehtävät joiden ratkaisut vaativat jokaista kuutta SQL-kielen lauseketta (SQL clause) tuottivat vaikeuksia\cite{Taipalus:2019:EFS:3287324.3287359}. Tavallisesti tehtävissä käytettiin kolmesta neljää eri lauseketta. Jotkin käsitteet ja konseptit toivat mukaansa itsellensä tyypillisiä virheitä mukaan tilastoihin. Esimerkiksi monitauluisissa kyselyissä esiintyi JOIN-kyselyille tyypillisiä syntaktisia- ja semanttisia virheitä. Yksittäinen yleisin ja pysyvin virhetyyppi nousi kokoamisfunktioiden (aggregation function) käytöstä, aina kun tehtävässä niitä tarvittiin. 


\section{Kurssi-menestyksen ennustaminen ohjelmointikursseilla}

Ohjelmointikurssit joiden tarkoitus on olla opiskelijoiden ensimmäinen kosketus ohjelmointiin ovat vaikeita huomattavalle osuudelle opiskelijoista. Tämä johtaa kursseilta pois jäämiseen ja hylättyihin arvosanoihin sekä heikkoihin arvosanoihin \cite{bergin2015using}. Aloittelijoiden ohjelmointikurssien läpipääsymäärä tutkimusaiheena on kasvattanut kasvattanut merkittävästi suosiotaan merkittävästi vuodesta 2009 alkaen\cite{hellas2018predicting}. Kiinnostuksen kasvusta huolimatta maailmanlaajuinen arvioitu läpipääsyjen määrä ohjelmoinnin aloituskursseilla oli noin 68\% vuonna 2014\cite{watson2014failure}. 

Vaikka halua opiskelijoiden auttamiseen onkin, on apua tarvitsevien löytäminen työlästä ja aikaa vievää. Ehdotettuja ratkaisuja ovat muun muassa ennakkoon tehtävät kyselyt\cite{watson2014no}. % Voi vaatia tarkemman sitaatin
Kyselyt ovat kuitenkin hyödyttömiä opiskelijamäärien ollessa yli sata jokaista kurssiohjaajaa kohden: lomakkeiden läpikäyminen vie liikaa aikaa, ja näin apua tarvitsevat löydetään liian myöhään. Tukea tarvitaan mahdollisimman varhaisessa vaiheessa jotta arvosanoihin voidaan vaikuttaa merkittävällä tavalla\cite{bergin2015using}. 

Ongelmaa on lähdetty ratkaisemaan automatisoimalla tunnistamisprosessia. Tunnistamisen autimatisointia on lähdetty ratkaisemaan pyrkimällä ennustamaan opiskelijoiden suoriutumista. Ohjelmointikurssien ennustamisen kohteena ovat usein kurssin loppuarvosana, tenttiarvosana sekä tehtävien kurssiarvosana. Yleisimpiä ennustamismenetelmiä ovat tilastolliset menetelmät\cite{hellas2018predicting}. Muita ohjelmointikurssien yhteydessä käytettäviä menetelmiä ovat muun muassa datalouhimis- ja koneoppimismenetelmät. Ohjelmointikursseilla data on usein kerätty ohjelmointiympäristön (IDE, Integrated Development Environment) keräämästä tiedosta. Tähän voi sisältyä ohjelmakoodin kääntämisen aikana kerätyt kirjaukset (ts. lokitukset), tai tilannekuvat (snapshot) koko ohjelmakoodista\cite{watson2013predicting, jadud2006methods, lagus2018transfer}.

Tilastollisten menetelmien yhteydessä piirteiden mallintamiseen on muutamassa tapauksessa käytetty tilakoneita. Näistä viitatuimpia ovat Error Quotient\cite{jadud2006methods}, Watson Score\cite{watson2013predicting} sekä Normalized Programming State Model (NPSM)\cite{carter2015normalized}. 

Error Quotient ja Watson Score rakentavat mallinsa oman ohjelmointikielen kääntäjän kääntämis-lokeista, ja näin pyrkivät ymmärtämään aloittelevien ohjelmoijien ohjelmointikääntämiskäyttäytymistä. Nämä mallit tarkastelevat opiskelijoiden virheellisiä sovelluksien kääntämisyrityksiä, ja vetävät johtopäätöksiä perättäisten kääntämiskertojen välillä. Error quotient on näistä kahdesta yksinkertaisempi. Se lähinnä tarkastelee perättäisten virheiden tyyppiä. Perättäisten virheiden lisäksi Watson Score ottaa käyttöönsä useampia parametreja. Watson Scoren:n käyttämä pääpiirre on virheiden korjaamiseen kuluva aika. Tätä aikaa verrataan samalla kurssilla oleviin opiskelijoihin, ja pisteytetään tämän mukaan. Jos aikaa kului enemmän kuin vertaisilla, lasketaan kyseisen opiskelijan pisteytystä.

NPSM lienee ottanut innoitusta Watson Score:sta. Kuten Watson Score, myös NPSM tarkkailee kulunutta aikaa. NPSM käyttää kuitenkin tarkempaa mallia ja käyttää ajankohtaisempaa dataa. Mallissa otetaan huomioon, oliko ohjelmakoodissa semanttisia- tai syntaktisia virheitä. Näin NPSM muodostaa holistisemman kuvan kunkin opiskelijan ohjelmakoodin tilasta. NPSM hyödynsi kääntämislokien lisäksi striimattuja otoksia opiskelijoiden koodista, joita opiskelijoiden ohjelmointiympäristät lähettivät, antaen tarkempaa tietoa esimerkiksi ohjelmakoodin syntaktisesta tilasta. 

Ennustavina menetelminä heikoin on Error Quotient, ja selkeästi vahvin on NPSM huomioitaessa selittävää varianssia\cite{carter2015normalized}. Error Quotient-mallista puuttuu liikaa tietoa ollakseen enemmän kuin suuntaa antava pisteytys\cite{jadud2006methods}. Vaikka Watson Score on tarkempi Error Quotient:iin nähden, on sen käyttämä data paljon suppeampaa verrattuna NPSM:iin\cite{carter2015normalized}. Carter kuitenkin huomautti mahdollisista eroista kurssiasetelmassa, ohjelmistoympäristöissä ja ohjelmointikielissä\cite{carter2015normalized}. 

Kullakin tilakoneella rakennettiin ennustava malli lineaarisella regressiolla. NPSM:a sovellettiin myös multivariate regressioon. Mielenkiintoinen tutkimisen aihe olisi soveltaa näiden tilakoneiden tuottamia parametreja monimutkaisempiin koneoppimismalleihin.

Opiskelijoiden ohjelmointikurssien menestyksen kontekstiin sovellettuja koneoppimistekniikoita on laaja kirjo. Kun kyseessä on akateemisen menestyksen ennustaminen, ovat suosituimpia tekniikoita lineaarinen regressio, ja luokittelualgoritmit\cite{hellas2018predicting}. Tämä heijastuu myös ohjelmointikurssien menestyksen ennustamiseen: tarkimpien menetelmien joukkoon kuuluvat muun muassa Naive Bayes\cite{bergin2015using}, tukivektorikoneet\cite{bergin2015using} sekä random forest\cite{lagus2018transfer}. Näistä jokainen on luokittelukoneoppmis-tekniikka. Neuroverkot ovat myös merkittävä tutkimuksen kohde ohjelmointikurssien menestyksen ennustamisessa\cite{Castro-Wunsch:2017:ENN:3017680.3017792}. 

Koneoppimisalgoritmien ongelmana on usein niiden joustamattomuus: ne on koulutettava datasta, joka on samanlaista kuin itse käytössä oleva data. Tämä pätee erityisesti klassisiin koneoppimisalgoritmeihin kuten esimerkiksi tukivektorikoneisiin ja logistiseen regressioon. Jos tätä ehtoa ei noudateta, ovat ennusteiden tarkkuudet huomattavan heikkoja. Jos koneoppimisalgoritmi koulutetaan kurssia varten ja kurssin aikana, todennäköisesti ongelmaa ei ole. Tavoiteltavaa on kuitenkin saada aikaiseksi malli, jolla on kohtuullinen tarkkuus edes saman kurssin eri toistokerroilla. Vaikka ohjelmointikurssilla kurssimateriaali, tavoitteet ja opeteltavat asiat pysyisivät samana vuodesta toiseen, on muuttuvia tekijöitä jokaisella iteraatiokerralla useita. Muun muassa osallistujien lähtötaso, opettajat sekä opetusmenetelmät. Muuttuvat tekijät pudottavat ennusteiden tarkkuutta merkittävästi. Saman kurssin eri toistoihin sovellettuja algoritmeja on tutkittu muun muassa neuroverkoilla\cite{Castro-Wunsch:2017:ENN:3017680.3017792} ja transfer learning\cite{lagus2018transfer}-tekniikalla.


\section{SQL-kurssien menestyksen ennustaminen}

Kurssimenestyksen ennustamisesta SQL-kurssien kontekstissa oli tausta-artikkelien keräämisen hetkellä varsin vähän. Ainoa artikkeli jossa suoraan puhuttiin koneoppimisen soveltamisesta oppilaiden menestyksen ennustamiseen SQL-kurssilla on Ahadin ym. kirjoittama "Students' Syntactic Mistakes in Writing Seven Differente Types of SQL Queries and its Application to Predicting Students' Success"\cite{Ahadi:2016:SSM:2839509.2844640}. Tutkimuksen lähestymistapana on luokitella opiskelijat onnistuneisiin ja epäonnistuneisiin heidän kääntämisyritysten perusteella PART-luokittelijalla.

PART luokittelija on sääntöpohjainen luokitteija joka perustuu C4.5 algoritmiin\cite{Ahadi:2016:SSM:2839509.2844640}. PART valittiin luokittelijaksi koska se hallitsee puuttuvat arvot, ja se tarjoaa selkeän esityksen generoiduista säännöistä. Itse kysymys johon luokittelijalla haettiin vastausta, oli selvittää mikä määrä kamppailua on hyviä ennuste sille että onnistuuko oppilas tuottamaan oikean kyselyn. Luokittelija koulutettiin ennustamaan että onnistuuko oppilas tuottamaan GROUP BY-kyselyn oikein.

Luokittelija sai syötteekseen 480 oppilaan yritykset yhteen tehtävään. Puolet oppilaista onnistui vastaamaan kysymykseen ja puolet eivät. Syötteeksi PART-luokittelijalle annettiin oppilaitten suorittamat kyselyt, sekä virheelliset että oikein menneet. Piirteitä valittiin (feature selection) korrelaatio-pohjaisilla menetelmillä ylisovittamisen välttämiseksi, piirteiden päällekkäisyyksien vähentämiseksi ja ennustustarkkuuden parantamiseksi. Koulutettu luokittelija validoitiin kymmenkertaisella cross-validation-menetelmällä. GROUP BY-kyselyn ennustamisessa saavutettiin 77\% tarkkuus.

Kuten edellissä kappaleessa todettiinkin, Ahadi ym. \cite{Ahadi:2016:SSM:2839509.2844640} muistuttavat eri koneopimisalgoritmien olevan voimakkaasti kontekstiriippuvaisia. Samalla tavoitteella koulutetut koneoppimismallit saavuttivat tarkkuuden 60\%:n ja 79\%:n väliltä.


\chapter{Koneoppimisen soveltaminen kurssi-dataan}

\begin{itemize}
    \item meneekö seuraava tehtävä oikein
    \item tuleva tenttiarvosana SQL tehtävissä
\end{itemize}

\section{Data}

Mitä dataa, mistä on saatu ja suunnitelmia käytöstä.


\section{Valitut mentelmät}

\begin{itemize}
    \item supervised learning valittu
\end{itemize}

\begin{itemize}
    \item mikä menetelmä, mukaanlukien matemaattinen selitys?
    \item mikä hyvää/huonoa?
    \item missä käytetty opetuksen kontekstissa?
    \item miksi valittu?
\end{itemize}

\subsection{Linear regression}

Lineaarinen regressio on yksinkertainen valvotun oppimisen (supervised learning) sovellus\cite{james2013ISLR}. Se on ollut käytössä pitkään ja se on edelleen laajasti käytetty tilastollinen menetelmä. Sen katsotaan olevan hyvä lähtökohta muiden koneoppimismenetelmien ymmärtämiseen. Iästään huolimatta lineaarinen regressio pystyy vastaamaan varsin joustavasti ilmiön ominaisuuksien välisiin kysymyksiin. 

Yksinkertainen lineaarinen regressio olettaa vasteen Y ja syötteen X välillä olevan lineaarinen suhde. Tämä suhde voidaan ilmaista seuraavasti. 

\begin{equation} \label{eq:1}
    Y = \beta_0 + \beta_1 X + \epsilon
\end{equation}

Yhtälössä $\epsilon$ kuvaa virhettä, jota ei yhtälöstä saada poistettua. Se syntyy arvoista, joita ei ole otettu tai pystytä ottamaan yhtälössä huomioon. Sitä voidaan ajatella satunnaisena kohinana jota datasta löytyy. Yhtälössä $\beta_0$ edustavaa leikkauspistettä, ja $\beta_1$ puolestaan on yhtälön kerroin. Ne ovat prosessin aloittamista tuntemattomia arvoja. Kun data on saatavilla, voidaan yhtälöstä laskea arvot $\hat{\beta_0}$ ja $\hat{\beta_1}$. Nämä arvoilla voidaan antaa ennuste vasteesta $\hat{y}$ annetule syötteelle $x$.

Yksinkertaisella lineaarisella regressiolla voidaan yrittää ennustaa vastetta yksittäisen ennustavan syötteen perusteella. Usein ennustavia parametreja on useita, ja näin yksinkertaista lineaarista regressiota laajennetaan moni-lineaariseksi regressioksi (multiple linear regression) seuraavasti.

\begin{equation} \label{eq:2}
    Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon
\end{equation}

Tässä $p$ kuvaa valittujen parametrien määrää, numeroidut syötteet $X_1\dots p$ kuvaavat kutakin valittua parametriä, joilla jokaisella on oma kertoimensa $\beta_1\dots p$.

Lineaarisella regressiolla saadaan nopeasti aikaiseksi suuntaa antavaa tietoa datan ominaisuuksista. Onko esimerkiksi syötteen ja vasteen välinen suhde lineaarinen, tai kuinka vahva syötteen ja vasteen välinen suhde. Vastaukset ovat kuitenkin vain suuntaa antavia. Tarkempina heikkouksina lineaariselle regressiolle mainitaan että sille ei ole luontevaa tapaa käsitellä kvalitatiivisia, eli laadullisia syötteitä\cite{james2013ISLR}. 

Akateemisen suoritumisen ennustamisessa lineaariset mallit, joihin lineaarinen regressio kuuluu, ovat käytetyimpiä. Lineaaristen mallien osuus käytetyistä ennustamisen menetelmistä oli vuonna 2018 31\%\cite{hellas2018predicting}. Lineaarista regressiota käyttivät muun muassa Carter ym. \cite{carter2015normalized} sekä Watson ym. \cite{watson2013predicting}.

Valitsin lineaarisen regression yhdeksi menetelmistä sen yksinkertaisuuden ja laajan käytön vuoksi. Yksinkertaisena mallina se tarjonnee jonkinlaisen vertailukohdan muita koneoppimismalleja soveltaessa.


\subsection{Support Vector Machine}

\begin{itemize}
    \item askel eteenpäin
    \item saanut hyvää palautetta
    \item kirjasta\cite{james2013ISLR}
\end{itemize}

Tukivektorikone on luokittelumenetelmä (classification), joka pyrkii jakamaan havaintoavaruuden kahteen osaan. Se laajentaa tukivektoriluokittelijaa mahdollistamalla epälineaarisen luokittelun kahden luokan välillä\cite{james2013ISLR}. Tukivektorikone saa syötteekseen $n \times p$ kokoisen matriisin $\textbf{X}$, jossa on $p$-ulottuvuutta, ja $n$-syötettä. Havaintojen luokat $y_1, \dots, y_n \in \{-1, 1\}$ ovat binäärisiä, joissa $-1$ ja $1$ ovat mahdollisia ennustettavia luokkia. Tukivektoriluokittelijan tavoin tukivektorikone pyrkii etsimään kahden luokan välisen rajan eli hypertason (hyperplane), ja saamaan mahdollisimman leveän marginaalin $M$ eri luokkiin kuuluvien havaintojen välille. Kullakin havainnolla $x_i$ on löysäysparametri (slack variable) $\epsilon_i$, joka kertoo havainnon sijainnin marginaaliin ja hypertasoon nähden seuraavasti. 

\begin{gather}
    \epsilon_i = 0\text{, niin havainto $x_i$ on oikealla puolella marginaalia}\\
    \epsilon_i > 0\text{, niin havainto $x_i$ on väärällä puolella marginaalia}\\
    \epsilon_i > 1\text{, niin havainto $x_i$ on väärällä puolella hypertasoa}
\end{gather}

Vektorit (eli havainnot) jotka ovat marginaalin sisällä tai ovat hypertason väärällä puolella, ovat nimeltään tukivektoreita. Sallittujen löysäysparametrien summaa määrää viritysparametri $C$, joka käytännössä valitaan ristivalidoinnilla (cross-validation). Löysäysparametrien summa $\sum^n_{i=1} \epsilon_i$ ei ylitä viritysparametria $C$. Viritysparametri säätelee tukivektoriluokittelijan ja tukivektorikoneen vinouma-varianssi tasapainoa (bias-variance trade-off). Siinä missä tukivektoriluokittelijaa käytetään kun havaintoavaruus on lineaarisesti jaettavissa, on tukivektorikone laajennettavissa muunkin muotoisilla funktioilla. Yleisesti tukivektorikone on ilmaistavissa seuraavasti.

\begin{equation}
    f(x) = \beta_0 + \sum_{i \in \textbf{S}} \alpha_i K(x, x_i)
\end{equation}

Jossa $\beta_0$ ilmaisee leikkauspistettä, $\textbf{S}$ sisältää havaintojen indeksit jotka ovat tukivektoreita, $\alpha_i$ on havainnon $x_i$ kerroin. Funktio $K$ on tukivektorikoneen ydinfunktio (kernel function). Tukivektorikoneen ydinfunktioksi voi asetella muun muassa polynomisen funktion, radiaalisen funktion tai tukivektoriluokittelijan lineaarisen funktion.

\begin{align}
    K(x_i, x_{i'}) &= \sum^p_{j=1} x_{i, j} x_{i', j} \text{ Lineaarinen ydinfunktio}\\
    K(x_i, x_{i'}) &= (1 + \sum^p_{j=1} x_{i, j} x_{i', j} )^d \text{ Polynominen funktio}\\
    K(x_i, x_{i'}) &= \exp{-\gamma \sum^p_{j=1}( x_{i, j} - x_{i', j} )^2} \text{ Radiaalinen funktio}
\end{align}

Parametri $p$ kertoo havaintoavaruuden ulottuvuuksien määrän. Polynomisessa funktiossa parametri $d$ on polynomisen funktion valittu aste. Radiaalisen funktion $\gamma$ on positiivinen vakio.

Ydin-funktioiden voimin tukivektorikoneet ovat laskennallisesti yksinkertaisia, ja pystyvät käsittelemään moniulotteista dataa. Lisäksi tukivektorikoneen kouluttaminen on varsin muistitehokasta koska ainoastaan tukivektorit, eli havainnot jotka ovat marginaalin sisäpuolella tai hypertason väärällä puolella osallistuvat tukivektorikoneen oppimiseen. Kuten lineaariregressiossa, tukivektorikone ei tarjoa todennäköisyyksiä luokkien ennustamisen tueksi. Datasta riippuen ydin-funktion valitseminen voi olla haastavaa, varsinkin datassa jossa on rajusti kohinaa. Suuren kohinan määrän läsnäollessa tukivektorikone saattaa ylisovittua kohinaa myötäillen, rampauttaen tukivektorikoneen suoriutumista.

Opetuksen kontekstissa Kentli ym. \cite{kentli2011svm} pyrkivät ennustamaan tekniikan alan kurssin opiskelijoiden suoriutumista tukivektorikoneen avulla. Lagus ym. \cite{lagus2018transfer} tekivät vastaavia ennusteita ohjelmointikurssilla. Artikkeli\cite{lagus2018transfer} tutki transfer learning-koneoppimismallien suoritumista, jossa tukivektorikone oli verrokkina. Tukivektorikoneet toimivat kunnioitettavalla tarkkuudella opiskelijat, jotka pääsivät kurssista läpi. Ennustetarkkuuksien keskiarvot vaihtelivat kurssien edetessä aina 82\% ja 87\% prosentin väliltä.

Tukivektorikone on lineaariregression jälkeen luonteva jatke. Se on hyvin toteutettavissa ja siitä on olemassa olevia toteutuksia. Tukivektorikone on hyvä yleistämään dataa ja pystyy antamaan hyviä ennusteita yksinkertaisuuteensa nähden.


\subsection{Random Forest}

Random forest-menetelmä on kokoelma päättelypuita, ja on laajennus pussittamisesta (bagging)\cite{james2013ISLR}. Pussittaminen perustuu bootstarpping-menetelmälle. Ajatuksena on tehdä useita satunnisia otoksia datasta. Tuotettu otos on alkuperäisen datan kokoinen. Kun datasta tehdään satunnaisia otoksia, voi muun muassa jokin syöte esiintyä useamman kerran bootstrapping-menetelllä tuotetussa datassa, ja osa taas voi jäädä otoksen ulkopuolelle. Datan ulkopuolisia syötteitä, eli "pussin ulkuopuolisia"  (out-of-bag) arvoja hyödynnetään tuotetun mallin virheen arvioimiseen. Pussittamisessa tämä satunnainen otos tehdään $B$-kertaa, ja jokaiselle data-otokselle tehdään oma päättelypuu. Regressio-ongelmassa päättelypuut antavat oman arvionsa, ja näistä arvioista lasketaan keskiarvo ennusteeksi.

\begin{equation}
    \hat{f}_{bag}(x) = \frac{1}{B} \sum^B_{b=1} \hat{f}^{*b}(x)
\end{equation}

Jos kyseessä on luokitteluongelma, saadaan luokan ennuste puiden enemmistöäänellä.\cite{james2013ISLR} 

Pussien määrä $B$:n kasvattaminen ei lisää ennusteiden ylisovittamista\cite{james2013ISLR}. Usein pussien määrän $B$ valinta tapahtuu siten, että sitä kasvatetaan kunnes luokitteluvirhe tasaantuu. 

Verrattuna yksittäiseen päättelypuuhun pussittaminen ennustusmenetelmänä on paljon tarkempi. Pussittamisen ongelmana on sama kuin yksittäisellä päätteylypuulla, eli syötteen korreloivat ennusteet ja vahvemmat ennusteet. Jos yksittäisellä ennusteella on vahva painoarvo ennusteessa, näyttävät pussittamisen päättelypuut samanlaisilta. Vastaavanlainen ongelma ilmenee korreloivissa ennusteissa luoden toistuvia rakenteita päättelypuissa.

Random forest ratkaisee ongelmaa vaikuttamalla logiikkaan, että millä valitaan kunkin puun haaran merkitsevä syötteen ennuste. Sen sijaan että syötteen jokaista ennustetta harkitaan kunkin haaran jakavaksi tekijäksi, valitaan jakava piirre vain satunnaisen $m$-kappaleen joukosta. Parametrin $m$ määrittelee usein $m \approx \sqrt{p}$, eli syötteiden ennusteiden määrän neliöjuuri. Näin random forest dekorreloi syötteiden ennusteita, ja saadaan suurempaa variaatiota luotujen päättelypuiden välille\cite{james2013ISLR}.

Ytimessään random forest on kokoelma päättelypuita. Sen ennusteet ovat huomattavasti merkittävästi varrattuna yksittäiseen päättelypuuhun\cite{james2013ISLR}. Koska päättelypuita on kokoelmassa useita ja satunnaisuus on olennainen osa mallin rakentamista, välttää random forest hyvin ylisovittamista\cite{james2013ISLR}. Usean päättelypuun käytön vuoksi random forest kuitenkin menettää yhden päättelypuun suurimmista eduista, joka on selkeä tulkittavuus\cite{james2013ISLR}. Päättelypuussa tuloksen tulkitseminen on yksinkertaista, sillä jokaista tulosta edustaa yksittäinen puun "lehti". Jokaisella lehdellä on selkeä reitti ja solmut, joita kautta kyseiseen lehteen on päädytty. Tämän sijasta random forest käyttää jokaisen puunsa antamaa ennustetta, joten ei tulos ole enään yhtä helposti luettavissa.

Opetukessa random forest on ollut mukana ennustamassa muun muassa hyvin ja heikosti suoriutuvia ohjelmointi-opiskelijoita. Yleisesti sitä on tarkasteltu muiden koneoppimismenetelmien rinnalla\cite{Ahadi:2015:EML:2787622.2787717}. Lisäksi se on ollut vertailussa tutkimuksessa transfer learning-menetelmän soveltuvuudesta samaan kontekstiin\cite{lagus2018transfer}. Molemmissa artikkeleissa random forest toimii poikkeuksellisella tarkkuudella.

Valitsin random forestin mukaan vertailuun koska se on mainittu erikseen ohjelmointiopetuksen kontekstissa. Haluan päästä näkemään saavutanko random forest-menetelmällä sen aiemman havaitun tarkkuuden.


\subsection{Naive Bayes}

Naive Bayes nimensä mukaisesti perustuu Bayesin teoreemaan. Se on yksinkertaistettu versio Bayes-optimoidusta luokittelijasta. Parametreina Bayes-optimoitu luokittelija saa havaintovektoreita ja ilmaisee todennäköisyyksinä että mihin luokkaan havainto kuuluu\cite{rish2001empirical}.

\begin{equation}
    f^*_i(\textbf{x}) = P(\textbf{X}=\textbf{x} | C=i) P(C=i)
\end{equation}

Bayes-optimoidun luokittelijan diskirminantissa funktiossa $\textbf{X}$ ilmaisee $j$-ulotteista havaintoa. $C$ on puolestaan luokka, johon pyritään havaintoa luokitella. Tavoitteena on siis ratkaista posterioritodennäköisyys Bayesin yhtälössä. Koska tavoitteena on kuitenkin luokitella havainto johonkin luokkaan $C$, on funktio ilmaistavissa seuraavasti\cite{rish2001empirical}:

\begin{equation}
    h^*(\textbf{x}) = \arg \max_i P(\textbf{X} = \textbf{x}|C=i) P(C = i)
\end{equation}

Tässä pyritään etsimään luokka $C = i$, joka maksimoi posterioritodennäköisyyden.

Koska kyse on ehdollisesta todennäköisyydestä, on havaintoken ulottuvuuksien määrän kasvaessa todennäköisyyden $P(\textbf{X} = \textbf{x}|C=i)$ suoraan laskennallisesti raskasta. Tämän vuoksi otetaan usein käyttöön estimaatteja tai yksinkertaistuksia, joista yksi esimerkki on naive Bayes-luokittelija. Naive Bayes tekee rajun yksinkertaistuksen olettaen että kaikki havaintojen piirteet ovat toisistaan riippumattomia. Tämä yksinkertaistaa kyseisen ehdollisen todennäköisyyden muotoon $P(\textbf{X}|C) = \Pi^n_{i=1} P(X_i | C)$, jolloin saadaan naive Bayesin funktio diskriminantissa muodossa:

\begin{equation}
    f^{NB}_i(\textbf{x}) = \Pi^n_{j=1} P(X_j=x_j | C=i) P(C=i)
\end{equation}

Tässä $j$ on havainnon indeksi, $i$ on luokan indeksi ja $n$ on havaintojen määrä. 

Rajusta yksinkertaistamisestaan huolimatta naive Bayes kilpailee suorituskyvyllään hienostuneempien koneoppimismenetelmien kanssa\cite{rish2001empirical}. Parhaiten se toimii havaintojen kanssa joiden piirteet ovat toisistaan riippumattomia, ja kun piirteet ovat toisistaan funktionaalisesti riippuvaisia. Naive Bayesin rajoituksen ovat nominaaliset koneoppimispiirteet, eli piirteet joilla on rajallinen määrä mahdollisia arvoja. Binääristen piirteiden kohdalla naive Bayes voi oppia vain lineaarisia suhteita, ja kun piirteiden mahdollisten arvojen määrä ylittää kaksi, on havaintoavaruden oltava polynomisesti jakautunut\cite{rish2001empirical}.

Opetuksessa naive Bayesia on käytetty esimerkiksi koulutuksellisen datan louhintaan\cite{bhardwaj2012data}, ja opiskelijoiden soveltuvuuden arviointiin kirjallisuuden opinnoissa\cite{hellas2018predicting}.

Naive Bayes on valittu menetelmänä mukaan sen saaman menestyksen ja yksinkertaisuuden vuoksi. Bayes-sukuiset menetelmät omaavat kokonaisen tilastotieteen koulukunnan, jonka opetusta löytyy myös Helsingin Yliopistossa. Haluan nähdä tämän yksinkertaisen Bayes-menetelmän sovelluksen käytännössä.


\subsection{Linear Discriminant Analysis}

\begin{itemize}
    \item koska Bayes? :D
    \item parhaiten normaalijakautuneen datan kanssa
    \item Bayes-luokittelijan jatke: se on arvio (approximation) Bayes-luokittelijasta
    \begin{itemize}
        \item priori $\pi_k$: todennäköisyys että satunnainen havainto kuuluu luokkaan $k$
        \begin{itemize}
            \item Helppo: se on luokkien välinen suhde
        \end{itemize}
        \item tiheysfunktio: $f_k(x) \equiv Pr(X = x | Y = k)$
        \begin{itemize}
            \item monimutkainen saavuttaa, jollei tehdä jakaumasta yksinkertaistuksia
        \end{itemize}
        \begin{itemize}
            \item Jos $f_k(x)$ on suuri, se kuuluu todennäköisesti luokkaan $k$
        \end{itemize}
        \item Bayesin kaava: $Pr(Y = k | X = x) = \frac{\pi_k f_k(x)}{\sum^K_{l=1} \pi_l f_l(x)}$
        \begin{itemize}
            \item jatkossa $p_k(X) = Pr(Y = k | X = x)$
        \end{itemize}
    \end{itemize}
    \item LDA olettaa datan jakauman seuraavan normaalijakaumaa
    \item käyttää
    \begin{itemize}
        \item normaalijakauman tiheysfunktiota
        \item luokkakohtaista keskiarvoa $\mu_k$
        \item luokkakohtaista varianssia $\sigma_k^2$
    \end{itemize}
    \item kun ennusteita $p$ on yksi, käytetään normaalia riheysfunktiota
    \begin{itemize}
        \item $f_k(x) = \frac{1}{\sqrt{2 \pi \sigma_k}} \exp{-\frac{1}{2 \sigma^2_k (x - \mu_k)^2}}$
    \end{itemize}
    \item tiheysfunktio sijoitetaan Bayesin kaavaan, ja otetaan siitä logaritmi ja näin saadaan diskriminantti funktio
    \begin{itemize}
        \item $\delta_k(x) = x \cdot \frac{\mu_k}{\sigma^2} - \frac{\mu^2_k}{2 \sigma^2} + \log(\pi_k)$
        \item tämä pyritään maksimoimaan
        \item luokka jolla tämä on suurin, on luokka johon havainto luokitellaan
    \end{itemize}
    \item koska ei tiedetä aitoja arvoja keskiarvoille $\mu_1, \dots, \mu_K$, prioreille $\pi_1, \dots, \pi_K$, eikä varianssille $\sigma^2$, LDA approksimoi Bayes luokittelijaa sijoittamalla aitojen arvojen tilalle arvioita aidoista arvoista havaintojen perusteella
    \begin{itemize}
        \item $\hat{\mu}_k = \frac{1}{n_k} \sum_{i:y_i = k} x_i$
        \item $\hat{\sigma}^2 = \frac{1}{n - K} \sum^K_{k=1} \sum_{i:y_i = k} (x_i - \hat{\mu}_k)^2$
        \item jos havaintojen luokan todennäköisyys tiedetään etukäteen, voidaan sitä käyttää suoraan priorina $\pi_1, \dots, \pi_K$ 
        \item jos havaintojen luokan esiintymistodennäköisyyttä ei tiedetä, voidaan priorina käyttää esiintyneiden havaintojen luokan suhdetta $\hat{\pi}_k = n_k / n$
    \end{itemize}
    \item näin saadaan diskriminantin funktion arvio, joka pyritään maksimoimaan luokan määrittelemiseksi
    \begin{itemize}
        \item $\hat{\delta_k(x)} = x \cdot \frac{\hat{\mu}_k}{\hat{\sigma}^2} - \frac{\hat{\mu}^2_k}{2 \hat{\sigma}^2} + \log(\hat{\pi}_k)$
    \end{itemize}
    \item tuloksena saadaan LINEAARINEN FUNKTIO $x$:n suhteen!
    \item useampaa havainnon $X = (X_1, X_2, \dots, X_p)$ ennusetta $p$ käyttäessä, käytetään
    \begin{itemize}
        \item monimuuttuja normaalijakauman tiheysfunktiota
        \item luokkakohtaista keskiarvovektoria
        \item yhteistä kovarianssi-matriisia
    \end{itemize}
    \item monimuuttuja nromaalijakauman tiheysfunktio $f(x)$
    \begin{itemize}
        \item $N(\mu_k, \pmb{\Sigma})$
        \item $\mu_k$ eli luokkakohtainen keskiarvo
        \item eli toisin sanoen $f(k) = \frac{1}{(2\pi)^{p/2} |\pmb{\Sigma}|^{1/2}} \exp{-\frac{1}{2} (x - \mu)^T \pmb{\Sigma}^{-1} (x - \mu)}$
        \item $X \sim N(\mu, \pmb{\Sigma})$
        \item $E(X) = \mu$
        \item $Cov(X) = \pmb{\Sigma}$
        \begin{itemize}
            \item $p \times p$ matriisi
        \end{itemize}
    \end{itemize}
    \item jälleen sijoitetaan tiheysfunktio Bayesin kaavaan, josta selvitetään jälleen diskriminantti funktio $\delta_k(x)$. Havainto sijoitetaan luokkaan jonka diskriminantti $\delta_k(x)$ funktio tuottaa suurimman arvon.
    \begin{itemize}
        \item $\delta_k(x) = x^T \pmb{\Sigma}^{-1} \mu_k - \frac{1}{2} \mu_k^T \pmb{\Sigma}^{-1} \mu_k + \log \pi_k$
    \end{itemize}
    \item arvojen estimaatit ovat samoja kuin yhden ennusteen $p$ tapauksessa, joista saadaan diskriminantin funktion $\hat{\delta}_k(x)$ arvio
\end{itemize}


\chapter{SQL-kurssin oppimenestyksen ennustamisen toteutus}

\section{Ohjelmakoodi}


\section{Prosessi, haasteet, yms.}


\section{Tulokset}


\chapter{Loppusanat\label{chapter:Loppusanat}}

It is good to conclude with some insightful discussion. 

% STEP 5:
% Uncomment the following lines and set your .bib file and desired bibliography style
% to make a bibliography with BibTeX.
% Alternatively you can use the thebibliography environment if you want to add all
% references by hand.

\cleardoublepage %fixes the position of bibliography in bookmarks
\phantomsection

\addcontentsline{toc}{chapter}{\bibname} % This lines adds the bibliography to the ToC
\bibliographystyle{abbrv} % numbering alphabetic order
\bibliography{bibliography}

\begin{appendices}
\myappendixtitle

\chapter{Code example\label{appendix:code}}
Program code can be added as appendix:
\begin{verbatim}
#!/bin/bash          
text="Hello World!"
echo $text
\end{verbatim}

\end{appendices}

\end{document}
